	at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:936)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:884)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:982)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)


scala> var input=sc.textFile("/user/cloudera/tweetCollect.txt")
15/07/08 13:20:12 INFO storage.MemoryStore: ensureFreeSpace(76616) called with curMem=316160, maxMem=311387750
15/07/08 13:20:12 INFO storage.MemoryStore: Block broadcast_2 stored as values to memory (estimated size 74.8 KB, free 296.6 MB)
input: org.apache.spark.rdd.RDD[String] = MappedRDD[7] at textFile at <console>:12

scala> var w=input.flatMap(l=>l.split(" ")).map(word=>(word,1)).cache()
w: org.apache.spark.rdd.RDD[(String, Int)] = MappedRDD[9] at map at <console>:14

scala> w.reduceByKey(_+_).saveAsTextFile("/user/cloudera/WordCount")
15/07/08 13:21:16 INFO mapred.FileInputFormat: Total input paths to process : 1
15/07/08 13:21:20 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/07/08 13:21:20 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/07/08 13:21:20 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/07/08 13:21:20 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/07/08 13:21:20 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/07/08 13:21:20 INFO spark.SparkContext: Starting job: saveAsTextFile at <console>:17
15/07/08 13:21:21 INFO scheduler.DAGScheduler: Registering RDD 10 (reduceByKey at <console>:17)
15/07/08 13:21:21 INFO scheduler.DAGScheduler: Got job 0 (saveAsTextFile at <console>:17) with 1 output partitions (allowLocal=false)
15/07/08 13:21:21 INFO scheduler.DAGScheduler: Final stage: Stage 0(saveAsTextFile at <console>:17)
15/07/08 13:21:21 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 1)
15/07/08 13:21:22 INFO scheduler.DAGScheduler: Missing parents: List(Stage 1)
15/07/08 13:21:22 INFO scheduler.DAGScheduler: Submitting Stage 1 (MapPartitionsRDD[10] at reduceByKey at <console>:17), which has no missing parents
15/07/08 13:21:24 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 1 (MapPartitionsRDD[10] at reduceByKey at <console>:17)
15/07/08 13:21:24 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
15/07/08 13:21:25 INFO scheduler.TaskSetManager: Starting task 1.0:0 as TID 0 on executor localhost: localhost (PROCESS_LOCAL)
15/07/08 13:21:25 INFO scheduler.TaskSetManager: Serialized task 1.0:0 as 2106 bytes in 56 ms
15/07/08 13:21:25 INFO executor.Executor: Running task ID 0
15/07/08 13:21:26 INFO storage.BlockManager: Found block broadcast_2 locally
15/07/08 13:21:27 INFO spark.CacheManager: Partition rdd_9_0 not found, computing it
15/07/08 13:21:27 INFO rdd.HadoopRDD: Input split: hdfs://quickstart.cloudera:8020/user/cloudera/tweetCollect.txt:0+16506
15/07/08 13:21:31 INFO storage.MemoryStore: ensureFreeSpace(216644) called with curMem=392776, maxMem=311387750
15/07/08 13:21:31 INFO storage.MemoryStore: Block rdd_9_0 stored as values to memory (estimated size 211.6 KB, free 296.4 MB)
15/07/08 13:21:31 INFO storage.BlockManagerInfo: Added rdd_9_0 in memory on 192.168.116.128:37218 (size: 211.6 KB, free: 296.8 MB)
15/07/08 13:21:31 INFO storage.BlockManagerMaster: Updated info of block rdd_9_0
15/07/08 13:22:10 INFO executor.Executor: Serialized size of result for 0 is 1312
15/07/08 13:22:10 INFO executor.Executor: Sending result for 0 directly to driver
15/07/08 13:22:10 INFO executor.Executor: Finished task ID 0
15/07/08 13:22:10 INFO scheduler.TaskSetManager: Finished TID 0 in 44961 ms on localhost (progress: 1/1)
15/07/08 13:22:10 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(1, 0)
15/07/08 13:22:10 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/07/08 13:22:10 INFO scheduler.DAGScheduler: Stage 1 (reduceByKey at <console>:17) finished in 45.501 s
15/07/08 13:22:10 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/07/08 13:22:10 INFO scheduler.DAGScheduler: running: Set()
15/07/08 13:22:10 INFO scheduler.DAGScheduler: waiting: Set(Stage 0)
15/07/08 13:22:10 INFO scheduler.DAGScheduler: failed: Set()
15/07/08 13:22:10 INFO scheduler.DAGScheduler: Missing parents for Stage 0: List()
15/07/08 13:22:10 INFO scheduler.DAGScheduler: Submitting Stage 0 (MappedRDD[13] at saveAsTextFile at <console>:17), which is now runnable
15/07/08 13:22:10 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 0 (MappedRDD[13] at saveAsTextFile at <console>:17)
15/07/08 13:22:10 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/07/08 13:22:10 INFO scheduler.TaskSetManager: Starting task 0.0:0 as TID 1 on executor localhost: localhost (PROCESS_LOCAL)
15/07/08 13:22:10 INFO scheduler.TaskSetManager: Serialized task 0.0:0 as 13047 bytes in 0 ms
15/07/08 13:22:10 INFO executor.Executor: Running task ID 1
15/07/08 13:22:10 INFO storage.BlockManager: Found block broadcast_2 locally
15/07/08 13:22:11 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
15/07/08 13:22:11 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/07/08 13:22:11 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: Started 0 remote fetches in 214 ms
15/07/08 13:22:17 INFO output.FileOutputCommitter: Saved output of task 'attempt_201507081321_0000_m_000000_1' to hdfs://quickstart.cloudera:8020/user/cloudera/WordCount/_temporary/0/task_201507081321_0000_m_000000
15/07/08 13:22:18 INFO spark.SparkHadoopWriter: attempt_201507081321_0000_m_000000_1: Committed
15/07/08 13:22:18 INFO executor.Executor: Serialized size of result for 1 is 825
15/07/08 13:22:18 INFO executor.Executor: Sending result for 1 directly to driver
15/07/08 13:22:18 INFO executor.Executor: Finished task ID 1
15/07/08 13:22:18 INFO scheduler.DAGScheduler: Completed ResultTask(0, 0)
15/07/08 13:22:18 INFO scheduler.DAGScheduler: Stage 0 (saveAsTextFile at <console>:17) finished in 7.241 s
15/07/08 13:22:18 INFO scheduler.TaskSetManager: Finished TID 1 in 7244 ms on localhost (progress: 1/1)
15/07/08 13:22:18 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/07/08 13:22:18 INFO spark.SparkContext: Job finished: saveAsTextFile at <console>:17, took 57.283688219 s

scala> var w=input.flatMap(l=>l.split(" ")).map(word=>(word,1)).cache()
w: org.apache.spark.rdd.RDD[(String, Int)] = MappedRDD[15] at map at <console>:14

scala> var c=w.reduceByKey(_+_).sortBy(_.y)
<console>:16: error: value sortBy is not a member of org.apache.spark.rdd.RDD[(String, Int)]
       var c=w.reduceByKey(_+_).sortBy(_.y)
                                ^

scala> var c=w.reduceByKey(_+_).sortByValue(false,1)
<console>:16: error: value sortByValue is not a member of org.apache.spark.rdd.RDD[(String, Int)]
       var c=w.reduceByKey(_+_).sortByValue(false,1)
                                ^

scala> var c=w.reduceByKey(_+_).map(item=>item.swap).sortByKey(false,1)
c: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[21] at sortByKey at <console>:16

scala> c.map(item=>item.swap).saveAsTextFile("/user/cloudera/group10/WordFreq")
org.apache.hadoop.fs.FileAlreadyExistsException: Parent path is not a directory: /user/cloudera/group10 group10
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.mkdirs(FSDirectory.java:1969)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:3696)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:3655)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3629)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:741)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:558)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2550)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2519)
	at org.apache.hadoop.hdfs.DistributedFileSystem$16.doCall(DistributedFileSystem.java:827)
	at org.apache.hadoop.hdfs.DistributedFileSystem$16.doCall(DistributedFileSystem.java:823)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:823)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:816)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1815)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:291)
	at org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)
	at org.apache.spark.SparkHadoopWriter.preSetup(SparkHadoopWriter.scala:63)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:817)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:724)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:643)
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1068)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:19)
	at $iwC$$iwC$$iwC.<init>(<console>:24)
	at $iwC$$iwC.<init>(<console>:26)
	at $iwC.<init>(<console>:28)
	at <init>(<console>:30)
	at .<init>(<console>:34)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:788)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1056)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:614)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:645)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:609)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:796)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:753)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
	at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:936)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:884)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:982)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.FileAlreadyExistsException): Parent path is not a directory: /user/cloudera/group10 group10
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.mkdirs(FSDirectory.java:1969)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:3696)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:3655)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3629)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:741)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:558)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy14.mkdirs(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy14.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:502)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2548)
	... 54 more


scala> c.map(item=>item.swap).saveAsTextFile("/user/cloudera/WordFreq")
15/07/08 15:10:18 INFO spark.SparkContext: Starting job: saveAsTextFile at <console>:19
15/07/08 15:10:18 INFO scheduler.DAGScheduler: Registering RDD 16 (reduceByKey at <console>:16)
15/07/08 15:10:18 INFO scheduler.DAGScheduler: Registering RDD 19 (map at <console>:16)
15/07/08 15:10:18 INFO scheduler.DAGScheduler: Got job 1 (saveAsTextFile at <console>:19) with 1 output partitions (allowLocal=false)
15/07/08 15:10:18 INFO scheduler.DAGScheduler: Final stage: Stage 2(saveAsTextFile at <console>:19)
15/07/08 15:10:18 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 3)
15/07/08 15:10:18 INFO scheduler.DAGScheduler: Missing parents: List(Stage 3)
15/07/08 15:10:18 INFO scheduler.DAGScheduler: Submitting Stage 4 (MapPartitionsRDD[16] at reduceByKey at <console>:16), which has no missing parents
15/07/08 15:10:18 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 4 (MapPartitionsRDD[16] at reduceByKey at <console>:16)
15/07/08 15:10:18 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
15/07/08 15:10:18 INFO scheduler.TaskSetManager: Starting task 4.0:0 as TID 2 on executor localhost: localhost (PROCESS_LOCAL)
15/07/08 15:10:18 INFO scheduler.TaskSetManager: Serialized task 4.0:0 as 2105 bytes in 1 ms
15/07/08 15:10:18 INFO executor.Executor: Running task ID 2
15/07/08 15:10:19 INFO storage.BlockManager: Found block broadcast_2 locally
15/07/08 15:10:19 INFO spark.CacheManager: Partition rdd_15_0 not found, computing it
15/07/08 15:10:19 INFO rdd.HadoopRDD: Input split: hdfs://quickstart.cloudera:8020/user/cloudera/tweetCollect.txt:0+16506
15/07/08 15:10:20 INFO storage.MemoryStore: ensureFreeSpace(216644) called with curMem=609420, maxMem=311387750
15/07/08 15:10:20 INFO storage.MemoryStore: Block rdd_15_0 stored as values to memory (estimated size 211.6 KB, free 296.2 MB)
15/07/08 15:10:20 INFO storage.BlockManagerInfo: Added rdd_15_0 in memory on 192.168.116.128:37218 (size: 211.6 KB, free: 296.5 MB)
15/07/08 15:10:20 INFO storage.BlockManagerMaster: Updated info of block rdd_15_0
15/07/08 15:10:24 INFO executor.Executor: Serialized size of result for 2 is 1312
15/07/08 15:10:24 INFO executor.Executor: Sending result for 2 directly to driver
15/07/08 15:10:24 INFO executor.Executor: Finished task ID 2
15/07/08 15:10:24 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(4, 0)
15/07/08 15:10:24 INFO scheduler.DAGScheduler: Stage 4 (reduceByKey at <console>:16) finished in 6.265 s
15/07/08 15:10:24 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/07/08 15:10:24 INFO scheduler.DAGScheduler: running: Set()
15/07/08 15:10:24 INFO scheduler.DAGScheduler: waiting: Set(Stage 2, Stage 3)
15/07/08 15:10:24 INFO scheduler.DAGScheduler: failed: Set()
15/07/08 15:10:24 INFO scheduler.TaskSetManager: Finished TID 2 in 6203 ms on localhost (progress: 1/1)
15/07/08 15:10:24 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/07/08 15:10:24 INFO scheduler.DAGScheduler: Missing parents for Stage 2: List(Stage 3)
15/07/08 15:10:25 INFO scheduler.DAGScheduler: Missing parents for Stage 3: List()
15/07/08 15:10:25 INFO scheduler.DAGScheduler: Submitting Stage 3 (MappedRDD[19] at map at <console>:16), which is now runnable
15/07/08 15:10:25 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 3 (MappedRDD[19] at map at <console>:16)
15/07/08 15:10:25 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
15/07/08 15:10:25 INFO scheduler.TaskSetManager: Starting task 3.0:0 as TID 3 on executor localhost: localhost (PROCESS_LOCAL)
15/07/08 15:10:25 INFO scheduler.TaskSetManager: Serialized task 3.0:0 as 2096 bytes in 0 ms
15/07/08 15:10:25 INFO executor.Executor: Running task ID 3
15/07/08 15:10:25 INFO storage.BlockManager: Found block broadcast_2 locally
15/07/08 15:10:25 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
15/07/08 15:10:25 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/07/08 15:10:25 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: Started 0 remote fetches in 24 ms
15/07/08 15:10:27 INFO executor.Executor: Serialized size of result for 3 is 1001
15/07/08 15:10:27 INFO executor.Executor: Sending result for 3 directly to driver
15/07/08 15:10:27 INFO executor.Executor: Finished task ID 3
15/07/08 15:10:27 INFO scheduler.DAGScheduler: Completed ShuffleMapTask(3, 0)
15/07/08 15:10:27 INFO scheduler.DAGScheduler: Stage 3 (map at <console>:16) finished in 1.927 s
15/07/08 15:10:27 INFO scheduler.DAGScheduler: looking for newly runnable stages
15/07/08 15:10:27 INFO scheduler.DAGScheduler: running: Set()
15/07/08 15:10:27 INFO scheduler.DAGScheduler: waiting: Set(Stage 2)
15/07/08 15:10:27 INFO scheduler.DAGScheduler: failed: Set()
15/07/08 15:10:27 INFO scheduler.TaskSetManager: Finished TID 3 in 1930 ms on localhost (progress: 1/1)
15/07/08 15:10:27 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/07/08 15:10:27 INFO scheduler.DAGScheduler: Missing parents for Stage 2: List()
15/07/08 15:10:27 INFO scheduler.DAGScheduler: Submitting Stage 2 (MappedRDD[25] at saveAsTextFile at <console>:19), which is now runnable
15/07/08 15:10:27 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 2 (MappedRDD[25] at saveAsTextFile at <console>:19)
15/07/08 15:10:27 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/07/08 15:10:27 INFO scheduler.TaskSetManager: Starting task 2.0:0 as TID 4 on executor localhost: localhost (PROCESS_LOCAL)
15/07/08 15:10:27 INFO scheduler.TaskSetManager: Serialized task 2.0:0 as 13437 bytes in 20 ms
15/07/08 15:10:27 INFO executor.Executor: Running task ID 4
15/07/08 15:10:27 INFO storage.BlockManager: Found block broadcast_2 locally
15/07/08 15:10:29 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
15/07/08 15:10:29 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/07/08 15:10:29 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: Started 0 remote fetches in 5 ms
15/07/08 15:10:32 INFO output.FileOutputCommitter: Saved output of task 'attempt_201507081510_0000_m_000000_4' to hdfs://quickstart.cloudera:8020/user/cloudera/WordFreq/_temporary/0/task_201507081510_0000_m_000000
15/07/08 15:10:32 INFO spark.SparkHadoopWriter: attempt_201507081510_0000_m_000000_4: Committed
15/07/08 15:10:32 INFO executor.Executor: Serialized size of result for 4 is 825
15/07/08 15:10:32 INFO executor.Executor: Sending result for 4 directly to driver
15/07/08 15:10:32 INFO executor.Executor: Finished task ID 4
15/07/08 15:10:32 INFO scheduler.DAGScheduler: Completed ResultTask(2, 0)
15/07/08 15:10:32 INFO scheduler.TaskSetManager: Finished TID 4 in 5272 ms on localhost (progress: 1/1)
15/07/08 15:10:32 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/07/08 15:10:32 INFO scheduler.DAGScheduler: Stage 2 (saveAsTextFile at <console>:19) finished in 5.278 s
15/07/08 15:10:32 INFO spark.SparkContext: Job finished: saveAsTextFile at <console>:19, took 14.745572738 s

scala> c.map(item=>item.swap).take(10).saveAsTextFile("/user/cloudera/WordHigh") 
<console>:19: error: value saveAsTextFile is not a member of Array[(String, Int)]
              c.map(item=>item.swap).take(10).saveAsTextFile("/user/cloudera/WordHigh")
                                              ^

scala> var d=c.map(item=>item.swap).take(10)
15/07/08 16:32:22 INFO spark.SparkContext: Starting job: take at <console>:18
15/07/08 16:32:24 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 140 bytes
15/07/08 16:32:25 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 140 bytes
15/07/08 16:32:25 INFO scheduler.DAGScheduler: Got job 2 (take at <console>:18) with 1 output partitions (allowLocal=true)
15/07/08 16:32:25 INFO scheduler.DAGScheduler: Final stage: Stage 5(take at <console>:18)
15/07/08 16:32:25 INFO scheduler.DAGScheduler: Parents of final stage: List(Stage 6)
15/07/08 16:32:25 INFO scheduler.DAGScheduler: Missing parents: List()
15/07/08 16:32:25 INFO scheduler.DAGScheduler: Submitting Stage 5 (MappedRDD[26] at map at <console>:18), which has no missing parents
15/07/08 16:32:26 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from Stage 5 (MappedRDD[26] at map at <console>:18)
15/07/08 16:32:27 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
15/07/08 16:32:27 INFO scheduler.TaskSetManager: Starting task 5.0:0 as TID 5 on executor localhost: localhost (PROCESS_LOCAL)
15/07/08 16:32:27 INFO scheduler.TaskSetManager: Serialized task 5.0:0 as 2383 bytes in 23 ms
15/07/08 16:32:27 INFO executor.Executor: Running task ID 5
15/07/08 16:32:28 INFO storage.BlockManager: Found block broadcast_2 locally
15/07/08 16:32:29 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329
15/07/08 16:32:29 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
15/07/08 16:32:29 INFO storage.BlockFetcherIterator$BasicBlockFetcherIterator: Started 0 remote fetches in 70 ms
15/07/08 16:32:30 INFO executor.Executor: Serialized size of result for 5 is 1170
15/07/08 16:32:30 INFO executor.Executor: Sending result for 5 directly to driver
15/07/08 16:32:30 INFO executor.Executor: Finished task ID 5
15/07/08 16:32:30 INFO scheduler.TaskSetManager: Finished TID 5 in 3300 ms on localhost (progress: 1/1)
15/07/08 16:32:30 INFO scheduler.DAGScheduler: Completed ResultTask(5, 0)
15/07/08 16:32:30 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/07/08 16:32:30 INFO scheduler.DAGScheduler: Stage 5 (take at <console>:18) finished in 3.657 s
15/07/08 16:32:31 INFO spark.SparkContext: Job finished: take at <console>:18, took 8.191648346 s
d: Array[(String, Int)] = Array((:,448), (,,364), ({,84), ("tweets",28), ("_id",28), ("$date",28), ("Language",28), ("Sensitivity",28), ("image2",28), (false},28))

scala> 
